{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pandas\n",
    "import numpy as numpy\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "######################### READING AND PREPROCESSING THE DATA ###########################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def read_data_from_csv_only_required_columns(directory_path, is_train_data):\n",
    "    if is_train_data:\n",
    "        required_columns = ['fare_amount', 'pickup_datetime', 'pickup_longitude', 'dropoff_longitude', 'pickup_latitude', 'dropoff_latitude', 'passenger_count']\n",
    "        train_data = pandas.read_csv(directory_path, nrows = 500000, usecols = required_columns)\n",
    "        return train_data\n",
    "    else:\n",
    "        test_data = pandas.read_csv(directory_path)\n",
    "        return test_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def convert_data_type_of_columns(data):\n",
    "    data[['pickup_datetime']] = pandas.to_datetime(data['pickup_datetime'].str.slice(0, 19), utc = True,\n",
    "                                                         format = '%Y-%m-%d %H:%M:%S')\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def drop_null_values(data):\n",
    "    data.dropna(inplace = True)\n",
    "    return data\n",
    "     \n",
    "    \n",
    "    \n",
    "###########################################################\n",
    "\n",
    "def remove_rows_with_fare_amount_less_than_zero(data):\n",
    "    #print(numpy.shape(train_data))\n",
    "    indices_of_rows_with_invalid_fare_amount = data.index[numpy.where(data['fare_amount'] < 2.50)]\n",
    "    data.drop(indices_of_rows_with_invalid_fare_amount, inplace = True)\n",
    "    return data\n",
    "\n",
    "############################################################\n",
    "\n",
    "'''city limits of NewYork'''\n",
    "def get_and_remove_rows_with_gps_points_outside_the_limits(data):\n",
    "    \n",
    "    rows_with_pickup_longitude_outside_limits = get_indices_of_rows_with_gps_values_outside_limits(data,\n",
    "                                                                                                 'pickup_latitude',\n",
    "                                                                                                 40, 42)\n",
    "    \n",
    "    rows_with_pickup_latitude_outside_limits = get_indices_of_rows_with_gps_values_outside_limits(data,\n",
    "                                                                                                 'pickup_longitude',\n",
    "                                                                                                 -75, -72)\n",
    "    \n",
    "    rows_with_pickup_longitude_outside_limits = get_indices_of_rows_with_gps_values_outside_limits(data,\n",
    "                                                                                                 'dropoff_latitude',\n",
    "                                                                                                 40, 42)\n",
    "    \n",
    "    rows_with_pickup_longitude_outside_limits = get_indices_of_rows_with_gps_values_outside_limits(data,\n",
    "                                                                                                 'dropoff_longitude',\n",
    "                                                                                                 -75, -72)\n",
    "    \n",
    "    \n",
    "    data = remove_rows_with_gps_points_outside_limits(data, rows_with_pickup_longitude_outside_limits)\n",
    "    data = remove_rows_with_gps_points_outside_limits(data, rows_with_pickup_latitude_outside_limits)\n",
    "    data = remove_rows_with_gps_points_outside_limits(data, rows_with_pickup_longitude_outside_limits)\n",
    "    data = remove_rows_with_gps_points_outside_limits(data, rows_with_pickup_longitude_outside_limits)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "############################################################\n",
    "\n",
    "def get_indices_of_rows_with_gps_values_outside_limits(data, column_name, lower, higher):\n",
    "    \n",
    "    indices = data.index[numpy.where((data[column_name] < lower) &\n",
    "                                                (data[column_name] > higher))]\n",
    "    \n",
    "    return indices\n",
    "\n",
    "############################################################\n",
    "\n",
    "def remove_rows_with_gps_points_outside_limits(data, row_indices):\n",
    "    \n",
    "    data.drop(row_indices, inplace = True)\n",
    "    return data\n",
    "\n",
    "    \n",
    "    \n",
    "###########################################################\n",
    "\n",
    "'''passenger count should be in the limits of 1 and 6'''\n",
    "def remove_rows_with_passenger_count_not_within_the_threshold(data):\n",
    "    indices_of_rows_with_invalid_passenger_count = data.index[numpy.where((data['passenger_count'] < 1) | \n",
    "                                                                                (data['passenger_count'] > 6))]\n",
    "    data.drop(indices_of_rows_with_invalid_passenger_count, inplace = True)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "###########################################################\n",
    "\n",
    "# I got this  function from https://www.kaggle.com/alvaroibrain/my-attempt-to-nyc-taxi-fare-prediction.\n",
    "def set_the_parameters_for_isInWater_func():\n",
    "    #Custom mask made quickly with photoshop. Black pixels will be considered sea,\n",
    "    newyork_img_mask = plt.imread('https://i.imgur.com/ov0cDqP.png')\n",
    "    #Bounds of the area\n",
    "    left_top = (-74.8, 41.1)\n",
    "    right_bottom = (-72.8, 40.092)\n",
    "    \n",
    "    #Delta distance in limits\n",
    "    distance_X = (right_bottom[0] - left_top[0])\n",
    "    distance_Y = (left_top[1] - right_bottom[1])\n",
    "    \n",
    "    return newyork_img_mask, distance_X, distance_Y, left_top, right_bottom\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "# I got this  function from https://www.kaggle.com/alvaroibrain/my-attempt-to-nyc-taxi-fare-prediction.\n",
    "def is_in_water(ny_img_mask, left_top, right_bottom, distance_X, distance_Y, coord):\n",
    "    \n",
    "    width = ny_img_mask.shape[1]\n",
    "    height = ny_img_mask.shape[0]\n",
    "    array_sea_color = numpy.array([0., 0. , 0.], dtype='float32')\n",
    "    \n",
    "    #Map delta distance to pixels\n",
    "    pix_x = ((coord[0] - left_top[0]) / distance_X) * width\n",
    "    pix_y = ((coord[1] - right_bottom[1]) / distance_Y) * height\n",
    "\n",
    "    if pix_x < 0 or pix_y < 0 or pix_x > width or pix_y > height:\n",
    "        return True #coord outside bounds\n",
    "    \n",
    "    #Get the color of the pixel\n",
    "    color = ny_img_mask[ height- int(pix_y), int(pix_x)]\n",
    "    \n",
    "    #Is in sea?  (compare color)\n",
    "    return numpy.array_equal(color, array_sea_color)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def get_indices_of_rows_where_coordinates_are_not_on_land(data, ny_img_mask, distance_X, distance_Y, left_top, right_bottom):\n",
    "        \n",
    "    bool_values_of_pickup_coordinates_not_on_land = data.apply(lambda row: is_in_water(ny_img_mask, left_top, right_bottom, \n",
    "                                                                            distance_X, distance_Y,\n",
    "                                                                            (row.pickup_longitude, \n",
    "                                                                             row.pickup_latitude)), axis = 1)\n",
    "    \n",
    "    bool_values_of_dropoff_coordinates_not_on_land = data.apply(lambda row: is_in_water(ny_img_mask, left_top, right_bottom, \n",
    "                                                                            distance_X, distance_Y,\n",
    "                                                                            (row.dropoff_longitude, \n",
    "                                                                             row.dropoff_latitude)), axis = 1)\n",
    "    \n",
    "\n",
    "    indices_of_rows_pickup_coordinates_not_on_land = list(data.index[numpy.where(bool_values_of_pickup_coordinates_not_on_land)])\n",
    "\n",
    "    indices_of_rows_drop_coordinates_not_on_land   = list(data.index[numpy.where(bool_values_of_dropoff_coordinates_not_on_land)])\n",
    "    \n",
    "    indices_of_rows_with_coordinates_not_on_land = list(set().union(indices_of_rows_pickup_coordinates_not_on_land,\n",
    "                                                             indices_of_rows_drop_coordinates_not_on_land))\n",
    "\n",
    "      \n",
    "    return numpy.array(indices_of_rows_with_coordinates_not_on_land)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def remove_rows_with_coordinates_not_on_land(data):\n",
    "    \n",
    "    newyork_img_mask, distance_X, distance_Y, left_top, right_bottom = set_the_parameters_for_isInWater_func()\n",
    "    indices_of_rows_with_coordinates_not_on_land = get_indices_of_rows_where_coordinates_are_not_on_land(data, newyork_img_mask, \n",
    "                                                                                                         distance_X, distance_Y, \n",
    "                                                                                                         left_top, right_bottom )\n",
    "    \n",
    "    data.drop(indices_of_rows_with_coordinates_not_on_land, inplace = True)\n",
    "    \n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "def get_the_target_variable_and_remove_useless_columns(data, is_train_data):\n",
    "    \n",
    "    if is_train_data:\n",
    "        train_Y = data['fare_amount']\n",
    "        train_data = data.drop(['fare_amount', 'pickup_datetime'], axis = 1)\n",
    "        return train_data, train_Y\n",
    "    \n",
    "    else:\n",
    "        data = data.drop(['key', 'pickup_datetime'], axis = 1)\n",
    "        return data\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "###########################################################\n",
    "\n",
    "def preprocess_data(data, is_train_data):\n",
    "    data = convert_data_type_of_columns(data)\n",
    "    data = drop_null_values(data)\n",
    "    if is_train_data:\n",
    "        data = remove_rows_with_fare_amount_less_than_zero(data)\n",
    "        \n",
    "    data = remove_rows_with_passenger_count_not_within_the_threshold(data)\n",
    "    data = remove_rows_with_coordinates_not_on_land(data)\n",
    "    data = get_and_remove_rows_with_gps_points_outside_the_limits(data)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#####################################################################################################################\n",
    "######################### ADDING MORE FEATURES TO THE DATA ###########################################################\n",
    "\n",
    "\n",
    "def get_absolute_diff_between_pickup_dropoff_values(pickup, dropoff):\n",
    "    return abs((pickup - dropoff))\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "'''x and y are tuples of length 2'''\n",
    "def calculate_euclidean_distance(x, y):\n",
    "    return numpy.sqrt(((x[0] - y[0]) ** 2) + ((x[1] - y[1]) ** 2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def calculate_manhattan_distance(x, y):\n",
    "    return ((y[0] - x[0]).abs() + (y[1] - x[1]).abs())\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "'''got this from here \n",
    "https://stackoverflow.com/questions/15736995/how-can-i-quickly-estimate-the-distance-between-two-latitude-longitude-points'''\n",
    "def calculate_haversine_distance(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    # Radius of earth in kilometers is 6371\n",
    "    km = 6371* c\n",
    "    return km\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def get_the_hour_of_booking(booking_datetime):\n",
    "    return booking_datetime.hour\n",
    "\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def get_the_day_of_booking(booking_datetime):\n",
    "    return booking_datetime.day\n",
    "\n",
    "\n",
    "###########################################################\n",
    "\n",
    "def add_columns_to_data(data):\n",
    "\n",
    "    data['euclidean_dist'] = calculate_euclidean_distance((data['pickup_longitude'], data['pickup_latitude']),\n",
    "                                (data['dropoff_longitude'], data['dropoff_latitude'])).astype(numpy.float32)\n",
    "\n",
    "    data['manhattan_dist'] = calculate_manhattan_distance((data['pickup_longitude'], data['pickup_latitude']),\n",
    "                                (data['dropoff_longitude'], data['dropoff_latitude'])).astype(numpy.float32)\n",
    "        \n",
    "    data['haversine_distance'] = data.apply(lambda row: calculate_haversine_distance(row.pickup_longitude, \n",
    "                                                                                                 row.pickup_latitude, \n",
    "                                                                                                 row.dropoff_longitude, \n",
    "                                                                                                 row.dropoff_latitude), \n",
    "                                                                                                            axis = 1)\n",
    "    \n",
    "    data['abs_longitude_diff'] = data.apply(lambda row: get_absolute_diff_between_pickup_dropoff_values(\n",
    "                                                                    row.pickup_longitude, row.dropoff_longitude), \n",
    "                                                                        axis = 1)\n",
    "    \n",
    "    data['abs_latitude_diff'] = data.apply(lambda row: get_absolute_diff_between_pickup_dropoff_values(\n",
    "                                                                    row.pickup_latitude, row.dropoff_latitude), \n",
    "                                                                        axis = 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# ###########################################################\n",
    "\n",
    "def find_correlation_between_columns_of_data(data):\n",
    "    corr_euclideanDist_fareAmount = data['euclidean_dist'].corr(data['fare_amount'])\n",
    "    corr_manhattanDist_fareAmount = data['manhattan_dist'].corr(data['fare_amount'])\n",
    "    #corr_hourOfBooking_fareAmount = data['hour_of_booking'].corr(data['fare_amount'])\n",
    "    corr_abs_long_diff_fareAmount = data['abs_longitude_diff'].corr(data['fare_amount'])\n",
    "    corr_abs_lat_diff_fareAmount = data['abs_latitude_diff'].corr(data['fare_amount'])\n",
    "    \n",
    "    print('Euclidean - fare - ' + str(corr_euclideanDist_fareAmount))\n",
    "    print('Manhattan - fare - ' + str(corr_manhattanDist_fareAmount))\n",
    "    print('Euclidean - fare - ' + str(corr_euclideanDist_fareAmount))\n",
    "    print('abs_long - fare - ' + str(corr_abs_long_diff_fareAmount))\n",
    "    print('abs_lat - fare - ' + str(corr_abs_lat_diff_fareAmount))\n",
    "    #print('Hour - fare - ' + str(corr_hourOfBooking_fareAmount))\n",
    "    \n",
    "\n",
    "                                                                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####################################################################################################################\n",
    "######################### STANDARDIZING THE DATA ###########################################################\n",
    "\n",
    "\n",
    "def standardize_the_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    data_standardized = scaler.fit_transform(data)\n",
    "    return data_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "########################################################################################################\n",
    "################################### LINEAR  REGRESSION #################################################\n",
    "\n",
    "def linear_regressor(train_X, train_Y):\n",
    "    train_X_standardized = standardize_the_data(train_X)\n",
    "    regressor = LinearRegression().fit(train_X_standardized, (numpy.array(train_Y).reshape(-1, 1)))    \n",
    "    return regressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################\n",
    "################################### RIDGE  REGRESSION ##################################################\n",
    "\n",
    "def perform_ridgeCV_for_best_alpha_and_predicting(train_X, train_Y):\n",
    "    \n",
    "    ridgeCV = RidgeCV(alphas = [0.03, 0.05, 0.1, 0.5, 1, 5, 10])\n",
    "    best_alpha_value = ridgeCV.fit(train_X, (numpy.array(train_Y).reshape(-1, 1)))\n",
    "    print('the best alpha is ' + str(best_alpha_value.alpha_))\n",
    "    return ridgeCV\n",
    "\n",
    "\n",
    "\n",
    "def perform_ridge_regression_on_the_data(train_X, train_Y):\n",
    "    train_X_standardized = standardize_the_data(train_X)\n",
    "    \n",
    "    ridgeCV_classifier = perform_ridgeCV_for_best_alpha_and_predicting(train_X_standardized, train_Y)\n",
    "    \n",
    "    \n",
    "    return ridgeCV_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################\n",
    "################################### RANDOM SEARCH AND GENERIC REGRESSION ##########################################\n",
    "\n",
    "\n",
    "\n",
    "def random_grid_search(train_X, train_Y, search_grid_params_dict, regressor):\n",
    "\n",
    "    \n",
    "    random_search_grid = RandomizedSearchCV(estimator = regressor, param_distributions = search_grid_params_dict,\n",
    "                                              n_iter = 5, cv = 2, verbose = 2, random_state = 23, n_jobs = -1)\n",
    "        \n",
    "    random_search_grid.fit(train_X, train_Y)\n",
    "    best_params = random_search_grid.best_params_\n",
    "    \n",
    "    \n",
    "    return best_params\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "\n",
    "def perform_regression(regressor, train_X, train_Y):\n",
    "    \n",
    "    regressor.fit(train_X, train_Y)\n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################\n",
    "################################### SUPPORT VECTOR REGRESSION ##########################################\n",
    "\n",
    "\n",
    "\n",
    "def set_params_for_grid_search():\n",
    "    \n",
    "    epsilon_value = [0.3, 0.5, 0.9]\n",
    "    C_value = [(pow(2, exponent)) for exponent in range(-3, 0)]\n",
    "    tolerance = [(pow(2, exponent)) for exponent in range(-10, -8)]\n",
    "    search_grid_params = {'epsilon' : epsilon_value,\n",
    "                          'C' : C_value,\n",
    "                          'tol' : tolerance\n",
    "                         }\n",
    "    return search_grid_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def perform_regression_on_data_using_SVR(train_X, train_Y):\n",
    "    \n",
    "    train_X_standardized = standardize_the_data(train_X)\n",
    "\n",
    "    SVR_regressor = LinearSVR()\n",
    "    \n",
    "    set_params_for_grid_search_SVR = set_params_for_grid_search()\n",
    "    best_params_dict = random_grid_search(train_X, train_Y, set_params_for_grid_search_SVR, SVR_regressor)\n",
    "    \n",
    "    print('best params for svm are ')\n",
    "    print(best_params_dict)\n",
    "    \n",
    "    \n",
    "    epsilon_value = best_params_dict['epsilon']\n",
    "    C_value = best_params_dict['C']\n",
    "    tolerance_value = best_params_dict['tol']\n",
    "\n",
    "    \n",
    "    regressor = LinearSVR(epsilon = epsilon_value, tol = tolerance_value, C = C_value, loss = 'squared_epsilon_insensitive',\n",
    "                                                       dual = False, random_state = 42)\n",
    "    regressor  = perform_regression(regressor, train_X_standardized, train_Y)\n",
    "    \n",
    "    \n",
    "    return regressor\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################################################\n",
    "################################### STOCHASTIC GRADIENT DESCENT ##########################################\n",
    "\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "def set_params_for_grid_search_SGD():\n",
    "    \n",
    "    loss_func = ['squared_loss', 'huber']\n",
    "    epsilon_value = [0.3, 0.5, 0.9]\n",
    "    alpha_value = [pow(10, exponent) for exponent in range(-4, -1)]\n",
    "    rate_of_learning = ['invscaling', 'adaptive']\n",
    "    \n",
    "    search_grid_params = {'loss' : loss_func,\n",
    "                          'alpha': alpha_value,\n",
    "                          'epsilon' : epsilon_value,\n",
    "                          'learning_rate' : rate_of_learning\n",
    "                         }\n",
    "    return search_grid_params\n",
    "\n",
    "\n",
    "\n",
    "# ###########################################################\n",
    "\n",
    "def perform_regression_on_data_using_SGD(train_X, train_Y):\n",
    "    \n",
    "    \n",
    "    train_X_standardized = standardize_the_data(train_X)\n",
    "    \n",
    "    SGD_regressor = SGDRegressor()\n",
    "    \n",
    "    params_for_grid_search_SGD = set_params_for_grid_search_SGD()\n",
    "    best_params_dict = random_grid_search(train_X_standardized, train_Y, \n",
    "                                                  params_for_grid_search_SGD, SGD_regressor)\n",
    "    \n",
    "    print('best params for SGD are')\n",
    "    print(best_params_dict)\n",
    "    \n",
    "    \n",
    "    loss_func = best_params_dict['loss']\n",
    "    epsilon_value = best_params_dict['epsilon']\n",
    "    rate_of_learning = best_params_dict['learning_rate']\n",
    "    alpha_value = best_params_dict['alpha']\n",
    "\n",
    "\n",
    "    \n",
    "    regressor = SGDRegressor(loss = loss_func, epsilon = epsilon_value,  learning_rate = rate_of_learning, \n",
    "                                                                                             random_state = 42)\n",
    "    \n",
    "    regressor = perform_regression(regressor, train_X_standardized, train_Y)\n",
    "    \n",
    "    \n",
    "    return regressor\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500000 entries, 0 to 499999\n",
      "Data columns (total 7 columns):\n",
      "fare_amount          500000 non-null float64\n",
      "pickup_datetime      500000 non-null object\n",
      "pickup_longitude     500000 non-null float64\n",
      "pickup_latitude      500000 non-null float64\n",
      "dropoff_longitude    499995 non-null float64\n",
      "dropoff_latitude     499995 non-null float64\n",
      "passenger_count      500000 non-null int64\n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 26.7+ MB\n",
      "None\n",
      "-----------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9914 entries, 0 to 9913\n",
      "Data columns (total 7 columns):\n",
      "key                  9914 non-null object\n",
      "pickup_datetime      9914 non-null object\n",
      "pickup_longitude     9914 non-null float64\n",
      "pickup_latitude      9914 non-null float64\n",
      "dropoff_longitude    9914 non-null float64\n",
      "dropoff_latitude     9914 non-null float64\n",
      "passenger_count      9914 non-null int64\n",
      "dtypes: float64(4), int64(1), object(2)\n",
      "memory usage: 542.2+ KB\n",
      "None\n",
      "*************************************\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 487430 entries, 0 to 499999\n",
      "Data columns (total 7 columns):\n",
      "fare_amount          487430 non-null float64\n",
      "pickup_datetime      487430 non-null datetime64[ns, UTC]\n",
      "pickup_longitude     487430 non-null float64\n",
      "pickup_latitude      487430 non-null float64\n",
      "dropoff_longitude    487430 non-null float64\n",
      "dropoff_latitude     487430 non-null float64\n",
      "passenger_count      487430 non-null int64\n",
      "dtypes: datetime64[ns, UTC](1), float64(5), int64(1)\n",
      "memory usage: 29.8 MB\n",
      "None\n",
      "-----------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9903 entries, 0 to 9913\n",
      "Data columns (total 7 columns):\n",
      "key                  9903 non-null object\n",
      "pickup_datetime      9903 non-null datetime64[ns, UTC]\n",
      "pickup_longitude     9903 non-null float64\n",
      "pickup_latitude      9903 non-null float64\n",
      "dropoff_longitude    9903 non-null float64\n",
      "dropoff_latitude     9903 non-null float64\n",
      "passenger_count      9903 non-null int64\n",
      "dtypes: datetime64[ns, UTC](1), float64(4), int64(1), object(1)\n",
      "memory usage: 618.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_data = read_data_from_csv_only_required_columns('/train.csv', True)\n",
    "test_data = read_data_from_csv_only_required_columns('/test.csv', False)\n",
    "print(train_data.info())\n",
    "print(\"-----------------\")\n",
    "print(test_data.info())\n",
    "\n",
    "\n",
    "train_data = preprocess_data(train_data, True)\n",
    "test_data = preprocess_data(test_data, False)\n",
    "print(\"*************************************\")\n",
    "print(train_data.info())\n",
    "print(\"-----------------\")\n",
    "print(test_data.info())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = add_columns_to_data(train_data)\n",
    "test_data = add_columns_to_data(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X, train_data_Y   = get_the_target_variable_and_remove_useless_columns(train_data, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype float32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype float32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "linear_regressor_ = linear_regressor(train_data_X,  train_data_Y)\n",
    "                                                                                           \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype float32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype float32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best alpha is 0.1\n"
     ]
    }
   ],
   "source": [
    "ridge_regressor = perform_ridge_regression_on_the_data(train_data_X,  train_data_Y)\n",
    "                                                                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype float32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype float32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  4.3min finished\n",
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params for svm are \n",
      "{'tol': 0.0009765625, 'epsilon': 0.3, 'C': 0.125}\n"
     ]
    }
   ],
   "source": [
    "SVR_regressor = perform_regression_on_data_using_SVR(train_data_X, train_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype float32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype float32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    5.8s finished\n",
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params for SGD are\n",
      "{'loss': 'squared_loss', 'learning_rate': 'invscaling', 'epsilon': 0.9, 'alpha': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "SGD_regressor = perform_regression_on_data_using_SGD(train_data_X, train_data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype float32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/envs/tensorflow-session/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype float32, int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "test_key = test_data['key']\n",
    "test_data = get_the_target_variable_and_remove_useless_columns(test_data, False)\n",
    "test_data_standardized = standardize_the_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_predicted_Y = SGD_regressor.predict(test_data_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVR_predicted_Y = SVR_regressor.predict(test_data_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_predicted_Y = ridge_regressor.predict(test_data_standardized)\n",
    "ridge_predicted_Y_list = [y[0] for y in ridge_predicted_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predicted_Y = linear_regressor_.predict(test_data_standardized)\n",
    "linear_predicted_Y_list = [y[0] for y in linear_predicted_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_the_submission_file(predicted_Y, key, regressor_name):\n",
    "    df = pandas.DataFrame({'key' : key, 'fare_amount' : (predicted_Y)}, columns = ['key', 'fare_amount'])\n",
    "    return df.to_csv((regressor_name + '_submissions.csv'), index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_submission = get_the_submission_file(list(linear_predicted_Y_list), list(test_key), 'linear')\n",
    "ridge_submission = get_the_submission_file(list(ridge_predicted_Y_list), list(test_key), 'ridge')\n",
    "SVR_submission = get_the_submission_file(list(SVR_predicted_Y), list(test_key), 'SVR')\n",
    "SGD_submission = get_the_submission_file(list(SGD_predicted_Y), list(test_key), 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
